{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYrpL8GMXidx",
        "outputId": "5c1887df-0bc9-4ac0-eee2-0719dae21ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch transformers datasets tensorflow accelerate huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "6TGhf7TzYDM0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Print sample data\n",
        "print(dataset[\"train\"][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G32hwQdVYW5d",
        "outputId": "f1e528d3-8e00-48ca-d118-091691bd9d1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnXq6BgiYu3E",
        "outputId": "c5340a1c-fa15-43d8-ae20-2bee0c5e3dab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '56be85543aeaaa14008c9065', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'What areas did Beyonce compete in when she was growing up?', 'answers': {'text': ['singing and dancing'], 'answer_start': [207]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_answers(example):\n",
        "    \"\"\"Convert nested answers dictionary into a single string.\"\"\"\n",
        "    if example[\"answers\"][\"text\"]:\n",
        "        example[\"answer\"] = example[\"answers\"][\"text\"][0]  # Take the first answer\n",
        "    else:\n",
        "        example[\"answer\"] = \"\"\n",
        "    return example\n",
        "\n",
        "# Apply preprocessing\n",
        "dataset = dataset.map(preprocess_answers)\n"
      ],
      "metadata": {
        "id": "drBmKdIRbPuR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example in dataset[\"train\"].select(range(5)):\n",
        "    print(f\"Question: {example['question']}\")\n",
        "    print(f\"Context: {example['context']}\")\n",
        "    print(f\"Answer: {example['answers']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hDnorhrcYsI",
        "outputId": "0df182b5-6b78-44aa-f04a-df98f58ebb22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: When did Beyonce start becoming popular?\n",
            "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Answer: {'text': ['in the late 1990s'], 'answer_start': [269]}\n",
            "\n",
            "Question: What areas did Beyonce compete in when she was growing up?\n",
            "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Answer: {'text': ['singing and dancing'], 'answer_start': [207]}\n",
            "\n",
            "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
            "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Answer: {'text': ['2003'], 'answer_start': [526]}\n",
            "\n",
            "Question: In what city and state did Beyonce  grow up? \n",
            "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Answer: {'text': ['Houston, Texas'], 'answer_start': [166]}\n",
            "\n",
            "Question: In which decade did Beyonce become famous?\n",
            "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Answer: {'text': ['late 1990s'], 'answer_start': [276]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a default padding token\n",
        "\n",
        "example = dataset[\"train\"][0]  # Take first example\n",
        "question = str(example[\"question\"]) if example[\"question\"] else \"\"\n",
        "context = str(example[\"context\"]) if example[\"context\"] else \"\"\n",
        "answer = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"\"\n",
        "\n",
        "inputs = tokenizer(question + \" \" + context, padding=\"max_length\", truncation=True, max_length=512)\n",
        "outputs = tokenizer(answer, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "print(f\"Input Length: {len(inputs['input_ids'])}, Label Length: {len(outputs['input_ids'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7hHTmOuccPh",
        "outputId": "84b30023-4f20-4202-d8ab-7e6ddee2e80a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Length: 512, Label Length: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(batch):\n",
        "    \"\"\"Handle batch processing correctly\"\"\"\n",
        "    # Process entire batches of text\n",
        "    questions = [str(q) for q in batch[\"question\"]]\n",
        "    contexts = [str(c) for c in batch[\"context\"]]\n",
        "    answers = [str(a) for a in batch[\"answer\"]]\n",
        "\n",
        "    # Create full input sequences\n",
        "    input_texts = [f\"{q} {c}\" for q, c in zip(questions, contexts)]\n",
        "\n",
        "    # Tokenize inputs in batch mode\n",
        "    inputs = tokenizer(\n",
        "        input_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"tf\"  # Important for TensorFlow compatibility\n",
        "    )\n",
        "\n",
        "    # Tokenize answers separately\n",
        "    outputs = tokenizer(\n",
        "        answers,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=50,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"].numpy(),\n",
        "        \"attention_mask\": inputs[\"attention_mask\"].numpy(),\n",
        "        \"labels\": outputs[\"input_ids\"].numpy()\n",
        "    }\n",
        "\n",
        "# Apply with adjusted batch size\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=32  # Reduce if you encounter memory issues\n",
        ")"
      ],
      "metadata": {
        "id": "VGZgyW1tfHqt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FvZP45bfxRB",
        "outputId": "361605e7-5da1-49f8-d3c8-09909db8ae08"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}, 'answer': 'in the late 1990s', 'input_ids': [2215, 750, 37361, 344, 923, 5033, 2968, 30, 37361, 32682, 402, 271, 13485, 9365, 829, 12, 49958, 50247, 8482, 135, 238, 45990, 73, 133, 240, 77, 325, 133, 103, 14, 20697, 12, 56, 1340, 12, 16706, 8, 357, 6286, 2693, 604, 11, 14745, 8, 318, 281, 1605, 14015, 11, 3496, 16002, 11, 1700, 9920, 290, 14549, 13, 18889, 290, 4376, 287, 6995, 11, 3936, 11, 673, 6157, 287, 2972, 13777, 290, 15360, 24174, 355, 257, 1200, 11, 290, 8278, 284, 16117, 287, 262, 2739, 6303, 82, 355, 1085, 14015, 286, 371, 5, 33, 2576, 12, 8094, 17886, 338, 5932, 13, 1869, 1886, 416, 607, 2988, 11, 6550, 6391, 9365, 829, 11, 262, 1448, 2627, 530, 286, 262, 995, 338, 1266, 12, 16473, 2576, 2628, 286, 477, 640, 13, 5334, 37009, 2497, 262, 2650, 286, 37361, 32682, 338, 8886, 5062, 11, 20419, 3481, 287, 5896, 357, 16088, 828, 543, 4920, 607, 355, 257, 12199, 6802, 8688, 11, 7366, 1936, 42235, 15434, 290, 8096, 262, 36229, 6964, 1802, 1271, 12, 505, 25287, 366, 34, 5918, 287, 5896, 1, 290, 366, 36534, 6387, 1911, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [259, 262, 2739, 6303, 82, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(batch):\n",
        "    \"\"\"Handle batch processing correctly\"\"\"\n",
        "    questions = [str(q) for q in batch[\"question\"]]\n",
        "    contexts = [str(c) for c in batch[\"context\"]]\n",
        "\n",
        "    # Handle missing answers safely\n",
        "    answers = [a[\"text\"][0] if \"text\" in a and a[\"text\"] else \"No answer\" for a in batch[\"answers\"]]\n",
        "\n",
        "    # Create full input sequences\n",
        "    input_texts = [f\"{q} {c}\" for q, c in zip(questions, contexts)]\n",
        "\n",
        "    # Tokenize inputs in batch mode\n",
        "    inputs = tokenizer(\n",
        "        input_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Tokenize answers separately\n",
        "    outputs = tokenizer(\n",
        "        answers,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=50\n",
        "    )\n",
        "\n",
        "    # Replace padding tokens in labels with -100 (ignored during loss computation)\n",
        "    labels = outputs[\"input_ids\"]\n",
        "    labels = [[token if token != tokenizer.pad_token_id else -100 for token in label] for label in labels]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "# Apply tokenization function again\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "3w8IPdbwglZH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary columns\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"id\", \"title\", \"context\", \"question\", \"answers\"])\n",
        ""
      ],
      "metadata": {
        "id": "UnuvBlFvoHsD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\"],  # Input features\n",
        "    label_cols=\"labels\",  # Explicitly set as a single string\n",
        "    shuffle=True,\n",
        "    batch_size=8\n",
        ")\n"
      ],
      "metadata": {
        "id": "vTvXF3iafvu3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR"
      ],
      "metadata": {
        "id": "RMAqP2z4mGPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\"],  # only features here\n",
        "    label_cols=[\"labels\"],                   # this becomes y\n",
        "    shuffle=True,\n",
        "    batch_size=8\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ik75KopnDGv",
        "outputId": "7f7a2e57-1023-4398-a549-2aa9e41b2f53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py:403: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from transformers import default_data_collator\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],  # Include labels in columns\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        "    collate_fn=default_data_collator,  # Good practice for GPT-2\n",
        "    drop_remainder=True\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wLIHDlzZZqG2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Tunning"
      ],
      "metadata": {
        "id": "ICBJgJHTi1Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, create_optimizer\n",
        "\n",
        "# Load the pre-trained GPT-2 model with language modeling head\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnfoCHY_i2K9",
        "outputId": "d98a55de-bed4-430d-f1c1-79030dc26d36"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "EPOCHS = 1  # Adjust as needed\n",
        "LEARNING_RATE = 3e-5\n",
        "BATCH_SIZE = 64\n",
        "STEPS_PER_EPOCH = len(train_dataset) // BATCH_SIZE\n",
        "\n",
        "# Create Optimizer\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=LEARNING_RATE,\n",
        "    num_train_steps=STEPS_PER_EPOCH * EPOCHS,\n",
        "    weight_decay_rate=0.01,\n",
        "    num_warmup_steps=0\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer)\n"
      ],
      "metadata": {
        "id": "JKAF0cx-i2kk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n"
      ],
      "metadata": {
        "id": "-XZj8pM5i_0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n2qS43BfjB5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ly-bMGMmjK0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cD4mk9JKjKxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0paG4kWnjKu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YLpEXsTqjKsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vwcelGhyjKqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7nRYI5YjKoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Don't edit\n",
        "#Expected\n",
        "Generated Answer: in the late 1990s\n"
      ],
      "metadata": {
        "id": "aDwCd6PUjLHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question, context):\n",
        "    input_text = f\"{question} {context}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
        "\n",
        "    output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Test Example\n",
        "question = \"When did Beyonce start becoming popular?\"\n",
        "context = \"Beyoncé rose to fame in the late 1990s as the lead singer of Destiny's Child.\"\n",
        "print(\"Generated Answer:\", generate_answer(question, context))\n"
      ],
      "metadata": {
        "id": "wOFTsjMLjNMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pr36XI0DlgW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpOvQ8-9lgTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Gml83fDlgRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YIODJqprlgOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fyffdc2UlgMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#New Code\n"
      ],
      "metadata": {
        "id": "i9XgeK5nlmbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Select half of the training data (e.g., first half)\n",
        "train_dataset = dataset[\"train\"]\n",
        "half_size = len(train_dataset) // 2\n",
        "\n",
        "# Option 1: Just take the first half\n",
        "train_subset = train_dataset.select(range(half_size))\n",
        "\n",
        "# OR Option 2: Shuffle first, then take half for a random subset\n",
        "train_subset = train_dataset.shuffle(seed=42).select(range(half_size))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Then apply your existing tokenization function\n",
        "tokenized_subset = train_subset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# Continue as usual\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
        "model.fit(train_tf_dataset, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "id": "GDZJ6NcXtWDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset and preprocess\n",
        "dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "#dataset = load_dataset(\"squad_v2\", split=\"train[:30%]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYe6axO6lokg",
        "outputId": "7baf7866-5f3c-42ce-a371-26e1de61e4c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Select half of the training data (e.g., first half)\n",
        "train_dataset = dataset\n",
        "half_size = len(train_dataset) // 4\n",
        "\n",
        "# Option 1: Just take the first half\n",
        "train_subset = train_dataset.select(range(half_size))\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x5-Dqsm2tunZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_answers(example):\n",
        "    \"\"\"Convert nested answers into a single string.\"\"\"\n",
        "    example[\"answer\"] = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"\"\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(preprocess_answers)\n",
        "#train_subset = train_subset.map(preprocess_answers)"
      ],
      "metadata": {
        "id": "5xzqK-hLlvn_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "MGcHPIaRmmZA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    \"\"\"Tokenize inputs with answer masking for training.\"\"\"\n",
        "    questions = [str(q) for q in batch[\"question\"]]\n",
        "    contexts = [str(c) for c in batch[\"context\"]]\n",
        "    answers = [a[\"text\"][0] if a[\"text\"] else \"\" for a in batch[\"answers\"]]\n",
        "\n",
        "    # Create full input text with structured format\n",
        "    input_texts = [\n",
        "        f\"Question: {q} Context: {c} Answer: {a}\"\n",
        "        for q, c, a in zip(questions, contexts, answers)\n",
        "    ]\n",
        "\n",
        "    # Tokenize entire sequence\n",
        "    tokenized = tokenizer(\n",
        "        input_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "\n",
        "    # Create labels mask\n",
        "    labels = tokenized[\"input_ids\"].copy()\n",
        "    for idx in range(len(input_texts)):\n",
        "        # Find approximate answer position using text format\n",
        "        prefix = f\"Question: {questions[idx]} Context: {contexts[idx]} Answer: \"\n",
        "        prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False, max_length=512, truncation=True)\n",
        "        answer_start = len(prefix_tokens)\n",
        "\n",
        "        # Mask everything before answer and padding\n",
        "        labels[idx, :answer_start] = -100\n",
        "        padding_mask = tokenized[\"input_ids\"][idx] == tokenizer.pad_token_id\n",
        "        labels[idx][padding_mask] = -100\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"labels\": labels.tolist()\n",
        "    }\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "#tokenized_datasets = train_subset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"id\", \"title\", \"context\", \"question\", \"answers\"])\n"
      ],
      "metadata": {
        "id": "greZJ4BVl9ZD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSpjlzZkxkZG",
        "outputId": "eb9b6ae9-25f4-4316-e06a-e309176f44d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['answer', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 130319\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['answer', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 11873\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "subset = DatasetDict({\n",
        "    \"train\": tokenized_datasets[\"train\"].shuffle(seed=42).select(range(int(0.01 * len(tokenized_datasets[\"train\"])))),\n",
        "    \"validation\": tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(int(0.01 * len(tokenized_datasets[\"validation\"]))))\n",
        "})"
      ],
      "metadata": {
        "id": "goFzLQglvtXk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL18xWUJ0Jwh",
        "outputId": "6f11b1a1-b184-45e1-81cb-faf7f576c5e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1303\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 118\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = subset"
      ],
      "metadata": {
        "id": "0d0U6Uwu0MqH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pemIKeYW2N6k",
        "outputId": "ccdea241-f2b6-4217-ecc8-ce149bbd48af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1303\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 118\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare TF dataset\n",
        "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "# Load model\n",
        "#model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "# In model loading\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\",\n",
        "    use_cache=False,  # Saves 20% memory\n",
        "    output_hidden_states=False,\n",
        "    output_attentions=False\n",
        ")\n",
        "\n",
        "# Training setup\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "model.compile(optimizer=optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLpIFqzMlmIM",
        "outputId": "30a53d97-1d50-4e72-ac46-700d1967c5fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "model.fit(train_dataset, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DURXLfU0lgJt",
        "outputId": "f56236b8-60a0-4ffd-bb31-d2e09a47a7c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CH0SmzTXpHix"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}